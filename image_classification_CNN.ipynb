{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipankarplug24/Image-Classification-using-CNN---Deep-Learning/blob/master/image_classification_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy7_Drfs5nCB",
        "colab_type": "text"
      },
      "source": [
        "Image Classification using **CNN** (**Convolution** **Neural** **Network**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAjiwV-0ok6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9e900691-bfc4-4e5b-b17b-73c4d55b65c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efkwNn1opB3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBYlgS5VpXcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5777e34a-d263-4688-d59d-9e6eabaed69a"
      },
      "source": [
        "#data preprocessing\n",
        "train_data = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "train_data = train_data.flow_from_directory('/content/drive/My Drive/cnn_dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8021 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-uNaiGsp3_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c7e71b2-6e51-474e-d930-2931682d3840"
      },
      "source": [
        "test_data = ImageDataGenerator(rescale = 1./255)\n",
        "test_data = test_data.flow_from_directory('/content/drive/My Drive/cnn_dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOZPSEK1qkS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D,Dense\n",
        "from tensorflow.keras.layers import Flatten\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLA5E-Qir_wg",
        "colab_type": "text"
      },
      "source": [
        "First **Convolution** Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B66i7fpcqxXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding first convolution layer\n",
        "cnn=Sequential()\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) \n",
        "cnn.add(MaxPool2D(pool_size=2, strides=2)) #pooling"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqP_Ed_sJiv",
        "colab_type": "text"
      },
      "source": [
        "Second **Convolution** Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6d2cSpNq793",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPool2D(pool_size=2, strides=2))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e65fYQNGsYad",
        "colab_type": "text"
      },
      "source": [
        "**Flattening**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbVX-BlRsX4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(Flatten())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9EVtSnTtFE8",
        "colab_type": "text"
      },
      "source": [
        "Full Connection (**Dense** Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OJM-f8XtAeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(Dense(128, activation='relu'))\n",
        "cnn.add(Dense(1, activation='sigmoid')) #output layer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO62obsmtXEi",
        "colab_type": "text"
      },
      "source": [
        "Compiling cnn model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x32gLMrtbya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX2wr8j3tkOw",
        "colab_type": "text"
      },
      "source": [
        "**Fitting** on Training data and **evaluationg** on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbNVvQlPtrLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8822d262-5c53-436e-bce7-d64d1390af77"
      },
      "source": [
        "cnn.fit(x = train_data, validation_data = test_data, epochs = 30)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "251/251 [==============================] - 5084s 20s/step - loss: 0.6705 - accuracy: 0.5836 - val_loss: 0.6476 - val_accuracy: 0.6567\n",
            "Epoch 2/30\n",
            "251/251 [==============================] - 55s 218ms/step - loss: 0.6143 - accuracy: 0.6685 - val_loss: 0.6156 - val_accuracy: 0.6767\n",
            "Epoch 3/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.5726 - accuracy: 0.7000 - val_loss: 0.5159 - val_accuracy: 0.7531\n",
            "Epoch 4/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.5421 - accuracy: 0.7257 - val_loss: 0.5176 - val_accuracy: 0.7501\n",
            "Epoch 5/30\n",
            "251/251 [==============================] - 56s 223ms/step - loss: 0.5210 - accuracy: 0.7386 - val_loss: 0.5109 - val_accuracy: 0.7536\n",
            "Epoch 6/30\n",
            "251/251 [==============================] - 54s 215ms/step - loss: 0.5034 - accuracy: 0.7545 - val_loss: 0.5114 - val_accuracy: 0.7571\n",
            "Epoch 7/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.4831 - accuracy: 0.7614 - val_loss: 0.5366 - val_accuracy: 0.7546\n",
            "Epoch 8/30\n",
            "251/251 [==============================] - 54s 215ms/step - loss: 0.4688 - accuracy: 0.7721 - val_loss: 0.5167 - val_accuracy: 0.7526\n",
            "Epoch 9/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.4482 - accuracy: 0.7888 - val_loss: 0.4777 - val_accuracy: 0.7751\n",
            "Epoch 10/30\n",
            "251/251 [==============================] - 54s 216ms/step - loss: 0.4399 - accuracy: 0.7905 - val_loss: 0.5393 - val_accuracy: 0.7611\n",
            "Epoch 11/30\n",
            "251/251 [==============================] - 55s 219ms/step - loss: 0.4293 - accuracy: 0.7979 - val_loss: 0.5340 - val_accuracy: 0.7446\n",
            "Epoch 12/30\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.4139 - accuracy: 0.8079 - val_loss: 0.4776 - val_accuracy: 0.7816\n",
            "Epoch 13/30\n",
            "251/251 [==============================] - 54s 216ms/step - loss: 0.3993 - accuracy: 0.8154 - val_loss: 0.4581 - val_accuracy: 0.7961\n",
            "Epoch 14/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.3785 - accuracy: 0.8308 - val_loss: 0.4695 - val_accuracy: 0.7941\n",
            "Epoch 15/30\n",
            "251/251 [==============================] - 53s 213ms/step - loss: 0.3731 - accuracy: 0.8341 - val_loss: 0.4741 - val_accuracy: 0.7961\n",
            "Epoch 16/30\n",
            "251/251 [==============================] - 55s 221ms/step - loss: 0.3633 - accuracy: 0.8392 - val_loss: 0.4681 - val_accuracy: 0.7946\n",
            "Epoch 17/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.3467 - accuracy: 0.8468 - val_loss: 0.4757 - val_accuracy: 0.7976\n",
            "Epoch 18/30\n",
            "251/251 [==============================] - 53s 213ms/step - loss: 0.3267 - accuracy: 0.8596 - val_loss: 0.4636 - val_accuracy: 0.8071\n",
            "Epoch 19/30\n",
            "251/251 [==============================] - 53s 213ms/step - loss: 0.3116 - accuracy: 0.8611 - val_loss: 0.5320 - val_accuracy: 0.7741\n",
            "Epoch 20/30\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.3066 - accuracy: 0.8657 - val_loss: 0.4645 - val_accuracy: 0.8001\n",
            "Epoch 21/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.2890 - accuracy: 0.8750 - val_loss: 0.4994 - val_accuracy: 0.7951\n",
            "Epoch 22/30\n",
            "251/251 [==============================] - 55s 219ms/step - loss: 0.2793 - accuracy: 0.8833 - val_loss: 0.4949 - val_accuracy: 0.8096\n",
            "Epoch 23/30\n",
            "251/251 [==============================] - 53s 210ms/step - loss: 0.2595 - accuracy: 0.8917 - val_loss: 0.5459 - val_accuracy: 0.7831\n",
            "Epoch 24/30\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.2500 - accuracy: 0.8938 - val_loss: 0.5997 - val_accuracy: 0.7666\n",
            "Epoch 25/30\n",
            "251/251 [==============================] - 55s 218ms/step - loss: 0.2393 - accuracy: 0.9019 - val_loss: 0.5391 - val_accuracy: 0.8001\n",
            "Epoch 26/30\n",
            "251/251 [==============================] - 54s 214ms/step - loss: 0.2305 - accuracy: 0.9024 - val_loss: 0.5556 - val_accuracy: 0.7936\n",
            "Epoch 27/30\n",
            "251/251 [==============================] - 54s 215ms/step - loss: 0.2197 - accuracy: 0.9099 - val_loss: 0.5549 - val_accuracy: 0.8001\n",
            "Epoch 28/30\n",
            "251/251 [==============================] - 56s 221ms/step - loss: 0.2025 - accuracy: 0.9157 - val_loss: 0.5828 - val_accuracy: 0.8046\n",
            "Epoch 29/30\n",
            "251/251 [==============================] - 53s 212ms/step - loss: 0.1882 - accuracy: 0.9238 - val_loss: 0.5457 - val_accuracy: 0.8141\n",
            "Epoch 30/30\n",
            "251/251 [==============================] - 55s 218ms/step - loss: 0.1903 - accuracy: 0.9203 - val_loss: 0.6326 - val_accuracy: 0.7986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa6d60f470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOlqo56JuDpp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57765c4b-5e19-4ca9-ae83-e026ac60394d"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq_sarS6u9rs",
        "colab_type": "text"
      },
      "source": [
        "Predicting on Single Image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx-5c0U_uAh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfa29698-f9ee-46ab-ad80-44f433f4468f"
      },
      "source": [
        "\n",
        "pred_img = image.load_img('/content/drive/My Drive/cnn_dataset/unknown_1.jpg', target_size = (64, 64))\n",
        "pred_img = image.img_to_array(pred_img)\n",
        "pred_img = np.expand_dims(pred_img, axis = 0)\n",
        "result = cnn.predict(pred_img)\n",
        "train_data.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeT06gmmGEhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "676185f4-17ae-4723-fd12-c5b937f3e1ee"
      },
      "source": [
        "\n",
        "pred_img = image.load_img('/content/drive/My Drive/cnn_dataset/cat.jpg', target_size = (64, 64))\n",
        "pred_img = image.img_to_array(pred_img)\n",
        "pred_img = np.expand_dims(pred_img, axis = 0)\n",
        "result = cnn.predict(pred_img)\n",
        "train_data.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U-lzP1CGIvM",
        "colab_type": "text"
      },
      "source": [
        "In both cases we got correct predictions"
      ]
    }
  ]
}